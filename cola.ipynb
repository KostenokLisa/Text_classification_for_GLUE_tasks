{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cola.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OjBBDNizk5m",
        "outputId": "bb0a9f0e-6fd6-49c6-8b7a-0d9c0905ebf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-RA-Gf1CpzQ",
        "outputId": "5bd6f6b2-968e-4cfb-83e8-10339bec1f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 36.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jDsNMgJihGw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import wget \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "KWSLVhB-lPNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():     \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available!\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gor-DU8sldTm",
        "outputId": "3f8aa2e4-9ca5-4aba-f9bf-ae1f2034e3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data               | Task                                      | Metric\n",
        "-------------------|-------------------------------------------|----------------\n",
        "CoLA               | Sentence is grammatical or not grammatical|Matthews\n",
        "SST-2              | Review is positive negative or neutral    |Accuracy\n",
        "RTE                | Sentence 1 -> Sentence 2?                 |Accuracy"
      ],
      "metadata": {
        "id": "rYYJpeq0wavm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing "
      ],
      "metadata": {
        "id": "2HrOcD-vCa8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cola_link = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "wget.download(cola_link, './cola_public_1.1.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LjfgFS8Mzxd9",
        "outputId": "b14c02e3-9fb1-4377-a8d4-ec2a085bfe26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./cola_public_1.1.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip cola_public_1.1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLtpcJRr0Muo",
        "outputId": "b0085e48-244e-4ccf-c0d8-fa8f0f81a988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cola_data_t = pd.read_csv(\"/content/cola_public/tokenized/in_domain_train.tsv\", delimiter=\"\\t\", names=['sentence_source', 'label', 'label_notes', 'sentence'], index_col=False)\n",
        "print(\"Total of train sentences\", cola_data_t.shape[0])\n",
        "print(\"Sample of tokenized sentence: \", cola_data_t[\"sentence\"][0])\n",
        "cola_data_t.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "b6J0OeEC4ral",
        "outputId": "cbc0816d-6157-4027-9b41-0978a8be3220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of train sentences 8551\n",
            "Sample of tokenized sentence:  our friends wo n't buy this analysis , let alone the next one we propose .\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sentence_source  label label_notes  \\\n",
              "0            gj04      1         NaN   \n",
              "1            gj04      1         NaN   \n",
              "2            gj04      1         NaN   \n",
              "3            gj04      1         NaN   \n",
              "4            gj04      1         NaN   \n",
              "\n",
              "                                            sentence  \n",
              "0  our friends wo n't buy this analysis , let alo...  \n",
              "1  one more pseudo generalization and i 'm giving...  \n",
              "2  one more pseudo generalization or i 'm giving ...  \n",
              "3   the more we study verbs , the crazier they get .  \n",
              "4         day by day the facts are getting murkier .  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72affff8-1cb4-428c-8788-6bfe3c61225f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>our friends wo n't buy this analysis , let alo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>one more pseudo generalization and i 'm giving...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>one more pseudo generalization or i 'm giving ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the more we study verbs , the crazier they get .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>day by day the facts are getting murkier .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72affff8-1cb4-428c-8788-6bfe3c61225f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72affff8-1cb4-428c-8788-6bfe3c61225f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72affff8-1cb4-428c-8788-6bfe3c61225f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cola_data_t = cola_data_t[[\"sentence\", \"label\"]]\n",
        "cola_data_t.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sViqjVQRKrvz",
        "outputId": "73914c8c-b7f1-40f6-f602-6f71826c671a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  label\n",
              "0  our friends wo n't buy this analysis , let alo...      1\n",
              "1  one more pseudo generalization and i 'm giving...      1\n",
              "2  one more pseudo generalization or i 'm giving ...      1\n",
              "3   the more we study verbs , the crazier they get .      1\n",
              "4         day by day the facts are getting murkier .      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-166e439e-4602-43c3-93b3-f623175de934\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>our friends wo n't buy this analysis , let alo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>one more pseudo generalization and i 'm giving...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>one more pseudo generalization or i 'm giving ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the more we study verbs , the crazier they get .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>day by day the facts are getting murkier .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-166e439e-4602-43c3-93b3-f623175de934')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-166e439e-4602-43c3-93b3-f623175de934 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-166e439e-4602-43c3-93b3-f623175de934');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_cola_data, test_cola_data, train_cola_labels, test_cola_labels = train_test_split(cola_data_t[\"sentence\"], cola_data_t[\"label\"], train_size=0.8)"
      ],
      "metadata": {
        "id": "-t2ubxvG1ehn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_cola_data, test_cola_data, train_cola_labels, test_cola_labels = train_test_split(cola_data[\"sentence\"], cola_data[\"label\"])"
      ],
      "metadata": {
        "id": "_uykG2bSZGZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_cola_data.shape)\n",
        "print(test_cola_data.shape)\n",
        "print(train_cola_labels.shape)\n",
        "print(test_cola_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMz-gg5Jc092",
        "outputId": "8ff04755-d6a5-4c99-c1fd-88738bb1e3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6840,)\n",
            "(1711,)\n",
            "(6840,)\n",
            "(1711,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline - BERT with default parameters as a feature extractor, Logistic Regression as a classifier"
      ],
      "metadata": {
        "id": "hLIAaIoiA3sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_class, tokenizer_class, weights):\n",
        "    tokenizer = tokenizer_class.from_pretrained(weights)\n",
        "    model = model_class.from_pretrained(weights)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "nNyw3DwEhCdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = load_model(ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvTOv7fNhUo0",
        "outputId": "4855f54d-5077-4fd1-ff0a-f1b8984b4fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = train_cola_data.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "tokenized_test = test_cola_data.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "metadata": {
        "id": "SjF5PjPUEYZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding(tokenized_data):\n",
        "    max_len = max([len(line) for line in tokenized_data.values])\n",
        "    padded_data = np.array([line + [0] * (max_len - len(line)) for line in tokenized_data.values])\n",
        "    #print(padded_cola_data.shape)\n",
        "    return padded_data"
      ],
      "metadata": {
        "id": "E3jGQU05fJ1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding\n",
        "padded_train_data = add_padding(tokenized_train)\n",
        "print(padded_train_data.shape)\n",
        "# attention mask\n",
        "mask_train = (padded_train_data > 0).astype(int)\n",
        "print(mask_train.shape)"
      ],
      "metadata": {
        "id": "R4ycsMmLOVkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding\n",
        "padded_test_data = add_padding(tokenized_test)\n",
        "print(padded_test_data.shape)\n",
        "# attention mask\n",
        "mask_test = (padded_test_data > 0).astype(int)\n",
        "print(mask_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JQRgF0SlNFD",
        "outputId": "30a55b82-d807-4709-9082-09a6b9cc9a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1711, 37)\n",
            "(1711, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_features_train = []\n",
        "batch_size = 128\n",
        "N = len(padded_train_data)\n",
        "for i in range(0, N, batch_size):\n",
        "    x_batch = torch.tensor(padded_train_data[i:i+batch_size]).to(device)\n",
        "    mask_batch = torch.tensor(mask_train[i:i+batch_size]).to(device)\n",
        "\n",
        "    # extract last hidden states to constuct feature matrix\n",
        "    with torch.no_grad():\n",
        "        batch_features = model(x_batch, attention_mask=mask_batch)[0][:, 0, :].cpu().numpy()\n",
        "        extracted_features_train.append(batch_features)"
      ],
      "metadata": {
        "id": "SsiOszD7kVAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_features_test = []\n",
        "batch_size = 128\n",
        "N = len(padded_test_data)\n",
        "for i in range(0, N, batch_size):\n",
        "    x_batch = torch.tensor(padded_test_data[i:i+batch_size]).to(device)\n",
        "    mask_batch = torch.tensor(mask_test[i:i+batch_size]).to(device)\n",
        "\n",
        "    # extract last hidden states to construct feature matrix\n",
        "    with torch.no_grad():\n",
        "        batch_features = model(x_batch, attention_mask=mask_batch)[0][:, 0, :].cpu().numpy()\n",
        "        extracted_features_test.append(batch_features)"
      ],
      "metadata": {
        "id": "U8KBKPf5hfzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = np.vstack(extracted_features_train)\n",
        "test_features = np.vstack(extracted_features_test)"
      ],
      "metadata": {
        "id": "HLCV_L-ijUir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEsNb7RLon9E",
        "outputId": "614bf60b-2b0b-469a-a4a1-2e0df020503e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6840, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Default Logistic Regression"
      ],
      "metadata": {
        "id": "k_ds_8JIVHin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import make_scorer"
      ],
      "metadata": {
        "id": "teOi7Xl8uHeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_model = LogisticRegression()\n",
        "clf_model.fit(train_features, train_cola_labels)\n",
        "pred_labels = clf_model.predict(train_features)\n",
        "pred_labels_test = clf_model.predict(test_features)"
      ],
      "metadata": {
        "id": "q4wt8nJhUC4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(matthews_corrcoef(train_cola_labels, pred_labels))\n",
        "print(matthews_corrcoef(test_cola_labels, pred_labels_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu3OvqecUEIW",
        "outputId": "911463a7-e850-479a-fedf-2d82eed3e54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.534427366046026\n",
            "0.38265855132679216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalized data + GridSearch (scoring for matthews correlation coef) "
      ],
      "metadata": {
        "id": "2e179uGVVYAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_train = scaler.fit_transform(train_features)\n",
        "scaled_test = scaler.transform(test_features)"
      ],
      "metadata": {
        "id": "nvRzVEXcIDdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100]}\n",
        "scorer = make_scorer(matthews_corrcoef)\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, scoring=scorer, refit=True)\n",
        "grid_search.fit(scaled_train, train_cola_labels)\n",
        "\n",
        "print('best C parameter: ', grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxcFhHntUUfb",
        "outputId": "b9e4b191-fe8f-4543-f49f-bf00fcb50075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best C parameter:  {'C': 0.01}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_model = LogisticRegression(C=0.01)\n",
        "clf_model.fit(scaled_train, train_cola_labels)\n",
        "pred_labels = clf_model.predict(scaled_train)\n",
        "pred_labels_test = clf_model.predict(scaled_test)"
      ],
      "metadata": {
        "id": "VHSw_MPdIhfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(matthews_corrcoef(train_cola_labels, pred_labels))\n",
        "print(matthews_corrcoef(test_cola_labels, pred_labels_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYVuGh1GUpBf",
        "outputId": "284c90c8-43da-4718-f95e-3b798f02716d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5214257478706297\n",
            "0.4089498908572961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Classifier"
      ],
      "metadata": {
        "id": "8letRcxP1Zdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = torch.tensor(train_features)\n",
        "train_labels = torch.tensor(train_cola_labels.values)\n",
        "train_dataset = TensorDataset(train_features, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "Dx9gfDvP3j_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = torch.tensor(test_features)\n",
        "test_labels = torch.tensor(test_cola_labels.values)\n",
        "test_dataset = TensorDataset(test_features, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "NDUWrrr-bDN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "        self.lin1 = nn.Linear(768, 128)\n",
        "        self.lin2 = nn.Linear(128, 32)\n",
        "        self.lin3 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        output = self.lin3(x)\n",
        "        return F.log_softmax(output)"
      ],
      "metadata": {
        "id": "F4NMRUpJXY3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomClassifier()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "hist = []\n",
        "\n",
        "# training cycle\n",
        "for t in range(30):\n",
        "    for x_batch, labels_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x_batch)\n",
        "        loss = loss_fn(preds, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(loss)\n",
        "    hist.append(loss.detach()) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgbQrKaQbBgz",
        "outputId": "58b22aa6-6e16-4805-e51e-f85756f08882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5222, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4930, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4685, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4427, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4233, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4031, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3790, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3690, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3474, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3311, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3251, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3108, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2804, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2560, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2390, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2097, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2058, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1866, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1695, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1686, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1509, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1241, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1376, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1271, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1088, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1090, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1336, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0882, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0865, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0962, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "k = np.arange(30)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()   \n",
        "plt.plot(k, hist) \n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "cPF3qAjSiR-F",
        "outputId": "d9961e7a-2a1b-43d2-b923-e9b8a1dc5901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, data_loader):\n",
        "    output = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_batch, labels_batch in data_loader:\n",
        "            preds = model(x_batch)\n",
        "            pred_labels = torch.argmax(preds, axis=1)\n",
        "            output.append(pred_labels.cpu().numpy())\n",
        "    return np.hstack(output)"
      ],
      "metadata": {
        "id": "23kKqBKITOOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels = predict(model, train_loader)\n",
        "pred_labels_test = predict(model, test_loader)\n",
        "print(\"Значение метрики на тренировочной выборке \", matthews_corrcoef(train_cola_labels.values, pred_labels))\n",
        "print(\"Значение метрики на тестовой выборке \", matthews_corrcoef(test_cola_labels.values, pred_labels_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HzhPXThVskG",
        "outputId": "c14b7d4e-d982-4d02-9224-ddb918d4285f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Значение метрики на тренировочной выборке  0.7664243336557055\n",
            "Значение метрики на тестовой выборке  0.3927212020089861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem: overfitting! We don't get profit for test metric using complex classifier. Solution: correction of parameters for whole model, not only for classifier"
      ],
      "metadata": {
        "id": "MBQqFWpl6v3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Fine-tuning "
      ],
      "metadata": {
        "id": "ZBB6VLhZkLLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cola_data, val_cola_data, train_cola_labels, val_cola_labels = train_test_split(train_cola_data, train_cola_labels, train_size=0.9)"
      ],
      "metadata": {
        "id": "t1lQnW92kRsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = train_cola_data.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "tokenized_val = val_cola_data.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "metadata": {
        "id": "cUweGo8trJZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding\n",
        "padded_train_data = add_padding(tokenized_train)\n",
        "print(padded_train_data.shape)\n",
        "# attention mask\n",
        "mask_train = (padded_train_data > 0).astype(int)\n",
        "print(mask_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzOEY8vvrJZa",
        "outputId": "a9c780c0-3843-473f-812f-cf15e60f9727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5540, 47)\n",
            "(5540, 47)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padding\n",
        "padded_val_data = add_padding(tokenized_val)\n",
        "print(padded_val_data.shape)\n",
        "# attention mask\n",
        "mask_val = (padded_val_data > 0).astype(int)\n",
        "print(mask_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db65fe61-833d-4164-e403-258cb6a73be2",
        "id": "590OkM1erJZb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(616, 40)\n",
            "(616, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ppb.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2, \n",
        "                                                          output_attentions = False,output_hidden_states = False)\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVTviE3B1arf",
        "outputId": "27754be4-5b41-4d43-f4bb-333872559ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total of model parameters\", len(list(model.parameters())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DoTK1zZoJB3",
        "outputId": "07de0330-53b9-46f6-8b6e-0690b593d9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of model parameters 201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters values(lr, total of epoch) are recommended in Hugging Face tutorials"
      ],
      "metadata": {
        "id": "-YaV4o_I7ii1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "N_train = padded_train_data.shape[0]\n",
        "N_val = padded_val_data.shape[0]\n",
        "batch_size = 32\n",
        "optimizer = ppb.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "\n",
        "for epoch in range(4):\n",
        "    model.train()\n",
        "    loss_epoch = 0\n",
        "    metric_epoch = 0\n",
        "    for i in range(0, N_train, batch_size):\n",
        "        model.zero_grad() \n",
        "        x_batch = torch.tensor(padded_train_data[i:i+batch_size]).to(device)\n",
        "        mask_batch = torch.tensor(mask_train[i:i+batch_size]).to(device)\n",
        "        labels_batch = torch.tensor(train_cola_labels.values[i:i+batch_size]).to(device) \n",
        "        loss = model(x_batch, attention_mask=mask_batch, labels=labels_batch).loss\n",
        "        logits = model(x_batch, attention_mask=mask_batch, labels=labels_batch).logits\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "\n",
        "        pred_labels = torch.argmax(logits, dim=1)\n",
        "        pred_labels = pred_labels.detach().cpu().numpy()\n",
        "        true_labels = train_cola_labels.values[i:i+batch_size]\n",
        "        metric_epoch += matthews_corrcoef(pred_labels, true_labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    n_batches = N_train // batch_size\n",
        "    print(\"Average train loss \", epoch, \"epoch: \", loss_epoch / n_batches)\n",
        "    print(\"Average train matthews_corrcoef \", epoch, \"epoch\", metric_epoch / n_batches)          \n",
        "\n",
        "    model.eval()\n",
        "    loss_epoch = 0\n",
        "    metric_epoch = 0\n",
        "    for i in range(0, N_val, batch_size):\n",
        "        x_batch = torch.tensor(padded_val_data[i:i+batch_size]).to(device)\n",
        "        mask_batch = torch.tensor(mask_val[i:i+batch_size]).to(device)\n",
        "        labels_batch = torch.tensor(val_cola_labels.values[i:i+batch_size]).to(device) \n",
        "        with torch.no_grad():\n",
        "            loss = model(x_batch, attention_mask=mask_batch, labels=labels_batch).loss\n",
        "            logits = model(x_batch, attention_mask=mask_batch, labels=labels_batch).logits\n",
        "            loss_epoch += loss.item()\n",
        "\n",
        "            pred_labels = torch.argmax(logits, dim=1)\n",
        "            pred_labels = pred_labels.detach().cpu().numpy()\n",
        "            true_labels = val_cola_labels.values[i:i+batch_size]\n",
        "            metric_epoch += matthews_corrcoef(pred_labels, true_labels)\n",
        "\n",
        "    n_batches = N_val // batch_size\n",
        "    print(\"Average val loss \", epoch, \"epoch: \", loss_epoch / n_batches)\n",
        "    print(\"Average  val matthews_corrcoef \", epoch, \"epoch\", metric_epoch / n_batches) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H-FtkhzNkFY",
        "outputId": "efaed5de-1d2d-4426-bcc8-bed139b30ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss  0 epoch:  0.526937290595446\n",
            "Average train matthews_corrcoef  0 epoch 0.3028175685796353\n",
            "Average val loss  0 epoch:  0.4476292282342911\n",
            "Average  val matthews_corrcoef  0 epoch 0.5500275831657288\n",
            "Average train loss  1 epoch:  0.33422032287182835\n",
            "Average train matthews_corrcoef  1 epoch 0.6691552809149051\n",
            "Average val loss  1 epoch:  0.45297014438792277\n",
            "Average  val matthews_corrcoef  1 epoch 0.6313707210939111\n",
            "Average train loss  2 epoch:  0.21864579697650982\n",
            "Average train matthews_corrcoef  2 epoch 0.8251818284829715\n",
            "Average val loss  2 epoch:  0.5856435067559543\n",
            "Average  val matthews_corrcoef  2 epoch 0.5710632784448405\n",
            "Average train loss  3 epoch:  0.18496821742138767\n",
            "Average train matthews_corrcoef  3 epoch 0.8660450737358011\n",
            "Average val loss  3 epoch:  0.5852166006439611\n",
            "Average  val matthews_corrcoef  3 epoch 0.628697527585067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of train and validation metrics proved that overfitting decreased, perfomance of the model improved"
      ],
      "metadata": {
        "id": "zBMXXTJt7yMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "N_test = len(padded_test_data)\n",
        "n_batch = N_test // batch_size \n",
        "total_metric = 0\n",
        "\n",
        "for i in range(0, N_test, batch_size):\n",
        "      x_batch = torch.tensor(padded_test_data[i:i+batch_size]).to(device)\n",
        "      mask_batch = torch.tensor(mask_test[i:i+batch_size]).to(device)\n",
        "      labels_batch = torch.tensor(test_cola_labels.values[i:i+batch_size]).to(device) \n",
        "      with torch.no_grad():\n",
        "          logits = model(x_batch, attention_mask=mask_batch).logits\n",
        "          pred_labels = torch.argmax(logits, dim=1)\n",
        "          pred_labels = pred_labels.detach().cpu().numpy()\n",
        "          true_labels = test_cola_labels.values[i:i+batch_size]\n",
        "          total_metric += matthews_corrcoef(pred_labels, true_labels)\n",
        "print(\"Average metric value on the test set:\", total_metric / n_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWYqICH3NtAF",
        "outputId": "a2993782-1c0d-4576-accd-890ff06e55df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average metric value on the test set: 0.5594159662907257\n"
          ]
        }
      ]
    }
  ]
}